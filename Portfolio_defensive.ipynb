{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk2z7zYrwweclaRIgvZ9HZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pierre-Alexandre01/Portfolio_defensive/blob/main/Portfolio_defensive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "from pathlib import Path\n",
        "from sklearn.covariance import LedoitWolf\n",
        "!pip install cvxpy osqp ecos\n",
        "!pip install ecos\n",
        "# (ECOS_BB comes with ecos; OSQP is great for QP)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: charts\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    HAS_MPL = True\n",
        "except Exception:\n",
        "    HAS_MPL = False\n",
        "\n",
        "def set_mpl_style():\n",
        "    \"\"\"\n",
        "    Minimal, publication-like styling without specifying colors.\n",
        "    \"\"\"\n",
        "    if not HAS_MPL:\n",
        "        return\n",
        "    plt.rcParams.update({\n",
        "        \"figure.figsize\": (8,5),\n",
        "        \"axes.titleweight\": \"bold\",\n",
        "        \"axes.titlesize\": 14,\n",
        "        \"axes.labelsize\": 11,\n",
        "        \"axes.spines.top\": False,\n",
        "        \"axes.spines.right\": False,\n",
        "        \"axes.spines.left\": True,\n",
        "        \"axes.spines.bottom\": True,\n",
        "        \"axes.grid\": True,\n",
        "        \"grid.linestyle\": \"--\",\n",
        "        \"grid.linewidth\": 0.5,\n",
        "        \"grid.alpha\": 0.5,\n",
        "        \"axes.axisbelow\": True,\n",
        "        \"legend.frameon\": False,\n",
        "        \"legend.fontsize\": 10,\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "        \"savefig.bbox\": \"tight\",\n",
        "    })\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except ImportError:\n",
        "    print(\"yfinance not installed. Install with: pip install yfinance\")\n",
        "    sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVF9DET58ptH",
        "outputId": "43a829a1-41c5-4439-a2b8-6b0fbf0c66d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (1.6.7)\n",
            "Requirement already satisfied: osqp in /usr/local/lib/python3.12/dist-packages (1.0.4)\n",
            "Collecting ecos\n",
            "  Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (3.2.8)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy) (1.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from osqp) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from osqp) (75.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp) (1.5.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->osqp) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy) (2.22)\n",
            "Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ecos\n",
            "Successfully installed ecos-2.0.14\n",
            "Requirement already satisfied: ecos in /usr/local/lib/python3.12/dist-packages (2.0.14)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from ecos) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.12/dist-packages (from ecos) (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# CONFIG — EDIT ME\n",
        "# -------------------------------\n",
        "CONFIG = {\n",
        "    \"universe_preset\": \"broad_multi_asset\",   # e.g. includes equities, ETFs, bonds, crypto\n",
        "    \"universe_file\": None,                    # don’t load from CSV\n",
        "    \"max_assets\": 30,                         # keep up to 60 tickers after filtering\n",
        "    \"min_history_days\": 500,                  # require ~3 years of history\n",
        "    \"min_coverage\": 0.80,                     # drop assets with too many missing values\n",
        "\n",
        "\n",
        "\n",
        "    \"start\": \"2010-01-01\",\n",
        "    \"end\": None,\n",
        "    \"risk_free_annual\": 0.02,\n",
        "    \"mc_paths\": 20000,\n",
        "    \"mc_days\": 252,\n",
        "    \"make_charts\": True,\n",
        "    \"show_plots\": True,\n",
        "    \"save_folder\": \"outputs\",\n",
        "\n",
        "    # Benchmark\n",
        "    \"benchmark_ticker\": \"AOR\",\n",
        "\n",
        "    # Which crises to analyze (only those within your data range will be used)\n",
        "    \"crisis_windows\": [\n",
        "        {\"name\":\"Global Financial Crisis\", \"start\":\"2007-10-01\", \"end\":\"2009-03-09\"},\n",
        "        {\"name\":\"US Downgrade / Euro wobble\", \"start\":\"2011-07-22\", \"end\":\"2011-10-03\"},\n",
        "        {\"name\":\"Taper Tantrum\", \"start\":\"2013-05-22\", \"end\":\"2013-06-24\"},\n",
        "        {\"name\":\"China/Commodities\", \"start\":\"2015-05-21\", \"end\":\"2016-02-11\"},\n",
        "        {\"name\":\"Q4 2018\", \"start\":\"2018-09-20\", \"end\":\"2018-12-24\"},\n",
        "        {\"name\":\"COVID crash\", \"start\":\"2020-02-19\", \"end\":\"2020-03-23\"},\n",
        "        {\"name\":\"2022 Bear\", \"start\":\"2022-01-03\", \"end\":\"2022-10-12\"},\n",
        "    ],\n",
        "        # --- Optimizer options ---\n",
        "    \"run_optimizer\": True,\n",
        "    \"optimizer\": {\n",
        "    \"mode\": \"mean_variance\",\n",
        "    \"risk_aversion\": 8.0,\n",
        "    \"target_vol\": None,\n",
        "    \"target_return\": None,\n",
        "    \"te_max\": None,            # drop TE, rely on group caps\n",
        "    \"long_only\": True,\n",
        "    \"w_min\": 0.0,\n",
        "    \"w_max\": 0.1,             # tighter single-name cap\n",
        "    \"cardinality\": None,\n",
        "    \"min_position\": 0.1,\n",
        "    \"random_samples\": 30000,\n",
        "    \"turnover_max\": 0.30,      # max absolute weight change sum (per rebalance), e.g. 30%\n",
        "    \"tc_bps\": 5,               # per-side trading cost in bps (used in objective penalty)\n",
        "    \"l2_ridge\": 1e-4,          # mild ℓ2 regularization on weights (stabilizes solutions)\n",
        "    \"prev_weights_file\": \"outputs/optimized_weights.csv\",  # used if present\n",
        "    },\n",
        "\n",
        "    \"walk_forward\": {\n",
        "    \"rebalance_freq\": \"M\",       # \"M\" monthly, \"Q\" quarterly\n",
        "    \"lookback_days\": 756,        # ~3y of data to estimate\n",
        "    \"start_buffer_days\": 1000,   # wait until enough history before first trade\n",
        "    \"tc_bps\": 5,                 # per-side cost\n",
        "}\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "8_Ns7mJX806E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Data classes\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class PortfolioResult:\n",
        "    ann_return: float\n",
        "    ann_vol: float\n",
        "    sharpe: float\n",
        "    mdd: float\n",
        "    var1d_95: float\n",
        "    cvar1d_95: float\n",
        "    var21d_95: float\n",
        "    cvar21d_95: float\n",
        "    summary_df: pd.DataFrame\n",
        "    weights: pd.Series\n",
        "    rets: pd.Series\n",
        "    prices: pd.DataFrame\n",
        "    cov_daily: pd.DataFrame\n",
        "    mu_daily: pd.Series"
      ],
      "metadata": {
        "id": "HXOay_XU86lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Helpers\n",
        "# -------------------------------\n",
        "\n",
        "TICKER_NAMES = {\n",
        "    \"AAPL\": \"Apple\",\n",
        "    \"MSFT\": \"Microsoft\",\n",
        "    \"AMZN\": \"Amazon\",\n",
        "    \"GOOG\": \"Alphabet (Google)\",\n",
        "    \"NVDA\": \"NVIDIA\",\n",
        "    \"META\": \"Meta Platforms\",\n",
        "    \"TSLA\": \"Tesla\",\n",
        "    \"BRK-B\": \"Berkshire Hathaway\",\n",
        "    \"JPM\": \"JPMorgan Chase\",\n",
        "    \"V\": \"Visa\",\n",
        "    \"XLI\": \"Industrial Select Sector ETF\",\n",
        "    \"XLK\": \"Tech Select Sector ETF\",\n",
        "    \"XLF\": \"Financial Select Sector ETF\",\n",
        "    \"XLV\": \"Health Care Select Sector ETF\",\n",
        "    \"XLY\": \"Consumer Discretionary ETF\",\n",
        "    \"XLP\": \"Consumer Staples ETF\",\n",
        "    \"SPY\": \"S&P 500 ETF\",\n",
        "    \"QQQ\": \"Nasdaq 100 ETF\",\n",
        "    \"IWM\": \"Russell 2000 ETF\",\n",
        "    \"TLT\": \"20y+ Treasury Bond ETF\",\n",
        "    \"GLD\": \"SPDR Gold Trust\",\n",
        "    \"SLV\": \"iShares Silver Trust\",\n",
        "    \"DBC\": \"Invesco Commodity Index ETF\",\n",
        "    \"USO\": \"United States Oil Fund\",\n",
        "    \"UNG\": \"United States Natural Gas Fund\",\n",
        "    \"VNQ\": \"Real Estate ETF\",\n",
        "    \"BTC-USD\": \"Bitcoin\",\n",
        "    \"ETH-USD\": \"Ethereum\",\n",
        "    \"SHY\": \"iShares 1–3 Year Treasury\",\n",
        "    \"LQD\": \"iShares iBoxx $ Inv Grade Corporate\",\n",
        "    \"HYG\": \"iShares iBoxx $ High Yield Corporate\",\n",
        "    \"VGK\": \"Vanguard FTSE Europe ETF\",\n",
        "    \"EFA\": \"iShares MSCI EAFE\",\n",
        "    \"EEM\": \"iShares MSCI Emerging Markets\",\n",
        "    \"EWJ\": \"iShares MSCI Japan\",\n",
        "    \"EWZ\": \"iShares MSCI Brazil\",\n",
        "    \"FXI\": \"iShares China Large-Cap\",\n",
        "    \"IEF\": \"iShares 7–10 Year Treasury\",\n",
        "    \"XLE\": \"Energy Select Sector ETF\",\n",
        "        # Leveraged / inverse equity\n",
        "    \"SQQQ\": \"ProShares UltraPro Short QQQ (−3x Nasdaq-100)\",\n",
        "    \"SPXS\": \"Direxion Daily S&P 500 Bear 3X\",\n",
        "    \"SPXL\": \"Direxion Daily S&P 500 Bull 3X\",\n",
        "    \"UPRO\": \"ProShares UltraPro S&P 500 (3x)\",\n",
        "    \"TQQQ\": \"ProShares UltraPro QQQ (3x)\",\n",
        "    \"SOXL\": \"Direxion Daily Semiconductor Bull 3X\",\n",
        "\n",
        "    # Volatility ETPs\n",
        "    \"VIXY\": \"ProShares VIX Short-Term Futures ETF\",\n",
        "    \"UVXY\": \"ProShares Ultra VIX Short-Term Futures (leveraged)\",\n",
        "\n",
        "    # Sectors / themes / commodities (levered)\n",
        "    \"SMH\":  \"VanEck Semiconductor ETF\",\n",
        "    \"XBI\":  \"SPDR S&P Biotech ETF\",\n",
        "    \"UGL\":  \"ProShares Ultra Gold (2x)\",\n",
        "    \"DGP\":  \"DB Gold Double Long ETN (2x)\",\n",
        "    \"NUGT\": \"Direxion Daily Gold Miners Bull (2x)\",\n",
        "    \"BOIL\": \"ProShares Ultra Bloomberg Natural Gas (2x)\",\n",
        "    # … extend as you like\n",
        "}\n",
        "# 0) Put these near the helpers\n",
        "CLASS_KEYS  = [\"equities\",\"bonds\",\"commodities\",\"crypto\",\"inverse_equity\",\"other\"]\n",
        "PRETTY_NAME = {\n",
        "    \"equities\": \"Equities\",\n",
        "    \"bonds\": \"Bonds\",\n",
        "    \"commodities\": \"Commodities\",\n",
        "    \"crypto\": \"Crypto\",\n",
        "    \"inverse_equity\": \"Equities (Hedge)\",\n",
        "    \"other\": \"Other\",\n",
        "}\n",
        "\n",
        "\n",
        "def _group_indices(cols):\n",
        "    eq = {\"SPY\",\"QQQ\",\"IWM\",\"XLK\",\"XLF\",\"XLV\",\"XLE\",\"XLY\",\"XLI\",\"XLP\",\n",
        "          \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOG\",\"META\",\"TSLA\",\"JPM\",\"V\",\"BRK-B\",\n",
        "          \"SMH\"}                     # add sector/industry ETFs you use\n",
        "    bonds = {\"TLT\",\"IEF\",\"LQD\",\"HYG\",\"SHY\"}\n",
        "    cmd = {\"GLD\",\"SLV\",\"DBC\",\"USO\",\"UNG\",\"DGP\",\"UGL\"}  # add levered gold\n",
        "    inverse_equity = {\"SQQQ\",\"SPXS\",\"SPXU\",\"SDOW\"}     # inverse/leveraged equity\n",
        "\n",
        "    eq_idx, bond_idx, cmd_idx, crypto_idx, inv_eq_idx = [],[],[],[],[]\n",
        "    for i,t in enumerate(cols):\n",
        "        if t in eq: eq_idx.append(i)\n",
        "        elif t in bonds: bond_idx.append(i)\n",
        "        elif t in cmd: cmd_idx.append(i)\n",
        "        if \"USD\" in t: crypto_idx.append(i)\n",
        "        if t in inverse_equity: inv_eq_idx.append(i)\n",
        "    return dict(equities=eq_idx, bonds=bond_idx, commodities=cmd_idx,\n",
        "                crypto=crypto_idx, inverse_equity=inv_eq_idx)\n",
        "\n",
        "def asset_class_members(weights_s: pd.Series) -> dict:\n",
        "    tickers = list(weights_s.index)\n",
        "    groups = _group_indices(tickers)\n",
        "\n",
        "    # index -> class\n",
        "    idx_to_class = {}\n",
        "    for cls, idxs in groups.items():\n",
        "        for i in idxs:\n",
        "            idx_to_class[i] = cls\n",
        "\n",
        "    # build buckets for *all* classes you support\n",
        "    buckets = {PRETTY_NAME[k]: [] for k in CLASS_KEYS}\n",
        "    for i, t in enumerate(tickers):\n",
        "        cls_key = idx_to_class.get(i, \"other\")\n",
        "        pretty = PRETTY_NAME.get(cls_key, \"Other\")\n",
        "        buckets[pretty].append((t, float(weights_s.loc[t])))\n",
        "\n",
        "    for k in buckets:\n",
        "        buckets[k].sort(key=lambda x: x[1], reverse=True)\n",
        "    return buckets\n",
        "\n",
        "def asset_class_breakdown(weights_s: pd.Series) -> pd.Series:\n",
        "    tickers = list(weights_s.index)\n",
        "    groups = _group_indices(tickers)\n",
        "\n",
        "    # index -> class\n",
        "    idx_to_class = {}\n",
        "    for cls, idxs in groups.items():\n",
        "        for i in idxs:\n",
        "            idx_to_class[i] = cls\n",
        "\n",
        "    # sum by class (include inverse_equity)\n",
        "    sums = {k: 0.0 for k in CLASS_KEYS}\n",
        "    for i, t in enumerate(tickers):\n",
        "        cls_key = idx_to_class.get(i, \"other\")\n",
        "        if cls_key not in sums:\n",
        "            sums[cls_key] = 0.0\n",
        "        sums[cls_key] += float(weights_s.loc[t])\n",
        "\n",
        "    s = pd.Series(sums)\n",
        "    s = s[s > 1e-10]  # drop zeros\n",
        "    return s.rename(index=PRETTY_NAME).sort_values(ascending=False)\n",
        "\n",
        "def plot_asset_class_pie(weights_s: pd.Series, out_path: Optional[str] = None, title: str = \"Portfolio by Asset Class\"):\n",
        "    if not HAS_MPL:\n",
        "        print(\"matplotlib not installed; skipping pie chart.\")\n",
        "        return\n",
        "    breakdown = asset_class_breakdown(weights_s)\n",
        "\n",
        "    # Optional: print a small table alongside the chart in the console\n",
        "    print(\"\\nAsset-class breakdown:\")\n",
        "    for k, v in breakdown.items():\n",
        "        print(f\"  {k:<12} {v:>6.2%}\")\n",
        "\n",
        "    # Pie chart\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.pie(\n",
        "        breakdown.values,\n",
        "        labels=[f\"{lbl} ({w:.1%})\" for lbl, w in zip(breakdown.index, breakdown.values)],\n",
        "        autopct=\"%1.1f%%\",\n",
        "        startangle=90\n",
        "    )\n",
        "    ax.set_title(title, fontweight=\"bold\")\n",
        "    ax.axis(\"equal\")  # make it a circle\n",
        "\n",
        "    if out_path:\n",
        "        plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "        print(f\"Saved asset-class pie → {out_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def ensure_weights(weights: List[float]) -> np.ndarray:\n",
        "    w = np.array(weights, dtype=float)\n",
        "    if not np.isclose(w.sum(), 1.0):\n",
        "        raise ValueError(f\"Weights must sum to 1.0 (got {w.sum():.6f}).\")\n",
        "    if (w < 0).any():\n",
        "        raise ValueError(\"Weights cannot be negative in this script.\")\n",
        "    return w\n",
        "\n",
        "def download_prices(tickers: List[str], start: str, end: Optional[str]) -> pd.DataFrame:\n",
        "    data = yf.download(tickers, start=start, end=end, auto_adjust=True, progress=False)[\"Close\"]\n",
        "    if isinstance(data, pd.Series):\n",
        "        data = data.to_frame()\n",
        "    data = data.dropna(how=\"all\").ffill().dropna()\n",
        "    missing = [t for t in tickers if t not in data.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing tickers in downloaded data: {missing}\")\n",
        "    return data\n",
        "\n",
        "def compute_portfolio_stats(prices: pd.DataFrame, weights: np.ndarray, rf_annual: float) -> PortfolioResult:\n",
        "    rets = prices.pct_change().dropna()\n",
        "    rets = _winsorize(rets, p=0.01)  # damp 1% tails\n",
        "\n",
        "    # Mean: EWMA (126d) + long-run anchor blend\n",
        "    ewma_mu = rets.ewm(span=126, min_periods=20).mean().iloc[-1]\n",
        "\n",
        "    # simple long-run anchors (daily) by rough asset class\n",
        "    anchors = []\n",
        "    for col in rets.columns:\n",
        "        u = col.upper()\n",
        "        if any(k in u for k in [\"SPY\",\"QQQ\",\"IWM\",\"XL\",\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOG\",\"META\",\"TSLA\",\"JPM\",\"V\",\"BRK-B\",\"EFA\",\"EEM\",\"EWJ\",\"VGK\",\"FXI\",\"EWZ\"]):\n",
        "            anch_ann = 0.06\n",
        "        elif any(k in u for k in [\"TLT\",\"IEF\",\"LQD\",\"HYG\",\"SHY\"]):\n",
        "            anch_ann = 0.02\n",
        "        elif any(k in u for k in [\"GLD\",\"SLV\",\"DBC\",\"USO\",\"UNG\"]):\n",
        "            anch_ann = 0.03\n",
        "        elif \"USD\" in u:  # BTC/ETH: set conservative or 0\n",
        "            anch_ann = 0.00\n",
        "        else:\n",
        "            anch_ann = 0.03\n",
        "        anchors.append((1 + anch_ann)**(1/252) - 1)\n",
        "    anchor_mu = pd.Series(anchors, index=rets.columns)\n",
        "\n",
        "    mu_daily = 0.5 * ewma_mu + 0.5 * anchor_mu\n",
        "\n",
        "    # Covariance: Ledoit–Wolf on winsorized returns\n",
        "    lw = LedoitWolf().fit(rets.values)\n",
        "    cov_daily = pd.DataFrame(lw.covariance_, index=rets.columns, columns=rets.columns)\n",
        "    port_rets = rets.dot(weights)\n",
        "\n",
        "    # Annualization\n",
        "    ann_return = (1 + port_rets).prod() ** (252 / len(port_rets)) - 1\n",
        "    ann_vol = port_rets.std() * np.sqrt(252)\n",
        "    rf_daily = (1 + rf_annual) ** (1 / 252) - 1\n",
        "    sharpe = ((port_rets.mean() - rf_daily) * 252) / (ann_vol if ann_vol != 0 else np.nan)\n",
        "\n",
        "    # Drawdown\n",
        "    equity = (1 + port_rets).cumprod()\n",
        "    peaks = equity.cummax()\n",
        "    dd = equity / peaks - 1\n",
        "    mdd = dd.min()\n",
        "\n",
        "    # VaR / CVaR (historical)\n",
        "    var1d_95 = -np.percentile(port_rets, 5)\n",
        "    cvar1d_95 = -port_rets[port_rets <= np.percentile(port_rets, 5)].mean()\n",
        "\n",
        "    # 21-day (approx month)\n",
        "    roll21 = port_rets.rolling(21).sum().dropna()\n",
        "    var21d_95 = -np.percentile(roll21, 5)\n",
        "    cvar21d_95 = -roll21[roll21 <= np.percentile(roll21, 5)].mean()\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        \"Metric\": [\n",
        "            \"Annualized return\", \"Annualized volatility\", \"Sharpe (rf=2%)\",\n",
        "            \"Max drawdown\", \"1-day VaR 95%\", \"1-day CVaR 95%\",\n",
        "            \"21-day VaR 95%\", \"21-day CVaR 95%\"\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            ann_return, ann_vol, sharpe, mdd, var1d_95, cvar1d_95, var21d_95, cvar21d_95\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    return PortfolioResult(\n",
        "        ann_return=ann_return, ann_vol=ann_vol, sharpe=sharpe, mdd=mdd,\n",
        "        var1d_95=var1d_95, cvar1d_95=cvar1d_95, var21d_95=var21d_95, cvar21d_95=cvar21d_95,\n",
        "        summary_df=summary, weights=pd.Series(weights, index=prices.columns, name=\"Weight\"),\n",
        "        rets=port_rets, prices=prices, cov_daily=cov_daily, mu_daily=mu_daily\n",
        "    )\n",
        "\n",
        "def _winsorize(df: pd.DataFrame, p: float = 0.01) -> pd.DataFrame:\n",
        "    lo = df.quantile(p)\n",
        "    hi = df.quantile(1 - p)\n",
        "    return df.clip(lower=lo, upper=hi, axis=1)\n",
        "\n",
        "def _cholesky_pd(M):\n",
        "    try:\n",
        "        return np.linalg.cholesky(M)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # nudge eigenvalues up\n",
        "        vals, vecs = np.linalg.eigh(M)\n",
        "        eps = 1e-10\n",
        "        vals[vals < eps] = eps\n",
        "        M_pd = (vecs * vals) @ vecs.T\n",
        "        return np.linalg.cholesky(M_pd)\n",
        "\n",
        "def monte_carlo(mu_daily, cov_daily, weights, paths, days, start_value=10_000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    L = _cholesky_pd(cov_daily.values)\n",
        "    means = mu_daily.values\n",
        "    Z = rng.standard_normal(size=(days, paths, len(means)))\n",
        "    sims = means + (Z @ L.T)\n",
        "    port_daily = sims @ weights\n",
        "    equity_curves = start_value * np.cumprod(1 + port_daily, axis=0)\n",
        "    final_returns = equity_curves[-1] / start_value - 1\n",
        "    stats = {\n",
        "        \"expected_1y_return_mc\": float(final_returns.mean()),\n",
        "        \"median_1y_return_mc\": float(np.median(final_returns)),\n",
        "        \"p5_1y_return_mc\": float(np.percentile(final_returns, 5)),\n",
        "        \"p95_1y_return_mc\": float(np.percentile(final_returns, 95)),\n",
        "    }\n",
        "    return equity_curves, final_returns, stats\n",
        "\n",
        "\n",
        "\n",
        "def stress_tests(prices: pd.DataFrame, weights: np.ndarray) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply simple 1-day shocks by asset class, using the same classification\n",
        "    as `_group_indices`. This guarantees consistency with your pie breakdown.\n",
        "    \"\"\"\n",
        "    cols = list(prices.columns)\n",
        "    w = pd.Series(weights, index=cols, dtype=float)\n",
        "\n",
        "    groups = _group_indices(cols)  # {'equities': [...], 'bonds': [...], 'commodities': [...], 'crypto': [...]}\n",
        "\n",
        "    def zero_shock() -> Dict[str, float]:\n",
        "        return {c: 0.0 for c in cols}\n",
        "\n",
        "    scenarios = []\n",
        "\n",
        "    # 1) Equities -20% (all tickers classified as equities)\n",
        "    sc_eq = zero_shock()\n",
        "    for i in groups.get(\"equities\", []):\n",
        "        sc_eq[cols[i]] = -0.20\n",
        "    scenarios.append((\"Equities -20% day\", sc_eq))\n",
        "\n",
        "    # 2) Gold -10% (only gold-like tickers inside commodities)\n",
        "    sc_gold = zero_shock()\n",
        "    for i in groups.get(\"commodities\", []):\n",
        "        c = cols[i].upper()\n",
        "        if any(k in c for k in [\"GLD\", \"GOLD\", \"UGL\", \"DGP\"]):\n",
        "            sc_gold[cols[i]] = -0.10\n",
        "    scenarios.append((\"Gold -10% day\", sc_gold))\n",
        "\n",
        "    # (Optional) If you want *all* commodities -10% instead of just gold, use this block instead:\n",
        "    # sc_cmd = zero_shock()\n",
        "    # for i in groups.get(\"commodities\", []):\n",
        "    #     sc_cmd[cols[i]] = -0.10\n",
        "    # scenarios.append((\"Commodities -10% day\", sc_cmd))\n",
        "\n",
        "    # 3) Crypto -30% (all tickers classified as crypto)\n",
        "    sc_crypto = zero_shock()\n",
        "    for i in groups.get(\"crypto\", []):\n",
        "        sc_crypto[cols[i]] = -0.30\n",
        "    scenarios.append((\"Crypto -30% day\", sc_crypto))\n",
        "\n",
        "    rows = []\n",
        "    for name, shock in scenarios:\n",
        "        pnl = sum(w[c] * shock[c] for c in cols)  # portfolio 1-day return from the shock\n",
        "        rows.append({\"Scenario\": name, \"Portfolio 1-day P&L (%)\": pnl * 100.0})\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def save_outputs(res: PortfolioResult, mc_stats: Dict, final_returns: np.ndarray, out_dir: str):\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    out_summary = f\"{out_dir}/summary.csv\"\n",
        "    df = res.summary_df.copy()\n",
        "    def _fmt(metric, value):\n",
        "        return f\"{value:.2f}\" if \"Sharpe\" in metric else f\"{value:.4%}\"\n",
        "    df[\"Formatted\"] = [_fmt(m, v) for m, v in zip(df[\"Metric\"], df[\"Value\"])]\n",
        "    df.to_csv(out_summary, index=False)\n",
        "\n",
        "    out_mc = f\"{out_dir}/mc_stats.json\"\n",
        "    with open(out_mc, \"w\") as f:\n",
        "        json.dump(mc_stats, f, indent=2)\n",
        "\n",
        "    out_dist = f\"{out_dir}/mc_final_returns.csv\"\n",
        "    pd.DataFrame({\"final_return\": final_returns}).to_csv(out_dist, index=False)\n",
        "\n",
        "    out_w = f\"{out_dir}/weights.csv\"\n",
        "    res.weights.to_csv(out_w, header=True)\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    for p in (out_summary, out_mc, out_dist, out_w):\n",
        "        print(\" -\", p)\n",
        "\n",
        "\n",
        "\n",
        "def make_charts(res: PortfolioResult, equity_curves: Optional[np.ndarray], out_dir: str, show_plots: bool,\n",
        "                bench_rets: Optional[pd.Series] = None):\n",
        "    if not HAS_MPL:\n",
        "        print(\"matplotlib not installed; skipping charts.\")\n",
        "        return\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    set_mpl_style()\n",
        "\n",
        "    # 1) Historical equity with (optional) benchmark overlay + drawdown shading\n",
        "    eq = (1 + res.rets).cumprod()\n",
        "\n",
        "    plt.figure()\n",
        "    eq.plot(label=\"Portfolio\", linewidth=2)\n",
        "\n",
        "    if bench_rets is not None:\n",
        "        # bench_rets are DAILY RETURNS. Align to portfolio dates and\n",
        "        # treat missing days as 0% return (no move), then cumprod.\n",
        "        aligned_bench = bench_rets.reindex(eq.index).fillna(0.0)\n",
        "        beq = (1.0 + aligned_bench).cumprod()\n",
        "        beq.plot(label=\"Benchmark\", linewidth=2)\n",
        "\n",
        "    # Drawdown shading for portfolio\n",
        "    peaks = eq.cummax()\n",
        "    dd = eq / peaks - 1\n",
        "    plt.fill_between(dd.index, 1 + dd.values, 1.0, alpha=0.12, step=\"pre\")\n",
        "\n",
        "    plt.title(\"HISTORICAL EQUITY (PORTFOLIO VS BENCHMARK)\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Equity (start=1.0)\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{out_dir}/historical_equity.png\")\n",
        "    if show_plots: plt.show()\n",
        "\n",
        "    if equity_curves is not None:\n",
        "        # 2) MC 1y return histogram + smooth line\n",
        "        final_returns = equity_curves[-1, :] / equity_curves[0, :] - 1\n",
        "        plt.figure()\n",
        "        n, bins, _ = plt.hist(final_returns, bins=60, alpha=0.8)\n",
        "        import numpy as _np\n",
        "        centers = (bins[:-1] + bins[1:]) / 2\n",
        "        smooth = _np.convolve(n, _np.ones(5)/5, mode=\"same\")\n",
        "        plt.plot(centers, smooth)\n",
        "        plt.title(\"MONTE CARLO: DISTRIBUTION OF 1-YEAR RETURNS\")\n",
        "        plt.xlabel(\"1-year return\"); plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(f\"{out_dir}/mc_1y_return_hist.png\")\n",
        "        if show_plots: plt.show()\n",
        "\n",
        "        # 3) MC equity fan (5–95% band + median)\n",
        "        pct = _np.percentile(equity_curves, [5, 50, 95], axis=1)\n",
        "        days = _np.arange(equity_curves.shape[0])\n",
        "        plt.figure()\n",
        "        plt.fill_between(days, pct[0], pct[2], alpha=0.2)\n",
        "        plt.plot(days, pct[1], linewidth=2)\n",
        "        plt.title(\"MONTE CARLO: EQUITY FAN (5–95% & MEDIAN)\")\n",
        "        plt.xlabel(\"Days\"); plt.ylabel(\"Portfolio Value (start=10,000)\")\n",
        "        plt.savefig(f\"{out_dir}/mc_fan_chart.png\")\n",
        "        if show_plots: plt.show()\n",
        "\n",
        "def download_benchmark(ticker: str, start: str, end: Optional[str]) -> pd.Series:\n",
        "    \"\"\"Download benchmark prices and return a Close price Series (aligned later).\"\"\"\n",
        "    px = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)[\"Close\"]\n",
        "    if isinstance(px, pd.DataFrame):\n",
        "        # yfinance can return a df if ticker resolves oddly; collapse to first column\n",
        "        px = px.iloc[:, 0]\n",
        "    return px.dropna().ffill()\n",
        "\n",
        "def compute_basic_stats_from_series(rets: pd.Series, rf_annual: float) -> Dict[str, float]:\n",
        "    \"\"\"Same stats as portfolio, but for any return series.\"\"\"\n",
        "    rets = rets.dropna()\n",
        "    rf_daily = (1 + rf_annual) ** (1/252) - 1\n",
        "    ann_return = (1 + rets).prod() ** (252 / len(rets)) - 1\n",
        "    ann_vol = rets.std() * np.sqrt(252)\n",
        "    sharpe = ((rets.mean() - rf_daily) * 252) / (ann_vol if ann_vol != 0 else np.nan)\n",
        "\n",
        "    # drawdown\n",
        "    eq = (1 + rets).cumprod()\n",
        "    dd = eq / eq.cummax() - 1\n",
        "    mdd = dd.min()\n",
        "\n",
        "    # VaR/CVaR examples (1d, 21d)\n",
        "    var1d_95 = -np.percentile(rets, 5)\n",
        "    cvar1d_95 = -rets[rets <= np.percentile(rets, 5)].mean()\n",
        "    roll21 = rets.rolling(21).sum().dropna()\n",
        "    var21d_95 = -np.percentile(roll21, 5) if len(roll21) > 0 else np.nan\n",
        "    cvar21d_95 = -roll21[roll21 <= np.percentile(roll21, 5)].mean() if len(roll21) > 0 else np.nan\n",
        "\n",
        "    return dict(\n",
        "        ann_return=ann_return, ann_vol=ann_vol, sharpe=sharpe, mdd=mdd,\n",
        "        var1d_95=var1d_95, cvar1d_95=cvar1d_95, var21d_95=var21d_95, cvar21d_95=cvar21d_95\n",
        "    )\n",
        "\n",
        "def compute_relative_metrics(port_rets: pd.Series, bench_rets: pd.Series, rf_annual: float) -> Dict[str, float]:\n",
        "    \"\"\"Alpha/Beta (vs. benchmark), Tracking Error, Information Ratio.\"\"\"\n",
        "    # Align\n",
        "    df = pd.concat([port_rets, bench_rets], axis=1).dropna()\n",
        "    df.columns = [\"port\", \"bench\"]\n",
        "\n",
        "    rf_daily = (1 + rf_annual) ** (1/252) - 1\n",
        "    # CAPM beta/alpha (daily → annualize alpha)\n",
        "    cov = np.cov(df[\"port\"], df[\"bench\"])[0, 1]\n",
        "    var_b = np.var(df[\"bench\"])\n",
        "    beta = cov / var_b if var_b != 0 else np.nan\n",
        "\n",
        "    # daily alphas using excess returns, then annualize\n",
        "    excess_p = df[\"port\"] - rf_daily\n",
        "    excess_b = df[\"bench\"] - rf_daily\n",
        "    # alpha_daily = E[excess_p - beta * excess_b]\n",
        "    alpha_daily = (excess_p - beta * excess_b).mean()\n",
        "    alpha_annual = (1 + alpha_daily) ** 252 - 1\n",
        "\n",
        "    # Tracking error & Information ratio\n",
        "    active = df[\"port\"] - df[\"bench\"]\n",
        "    te_annual = active.std() * np.sqrt(252)\n",
        "    info_ratio = (df[\"port\"].mean() - df[\"bench\"].mean()) * np.sqrt(252) / (active.std() if active.std() != 0 else np.nan)\n",
        "\n",
        "    return dict(beta=beta, alpha_annual=alpha_annual, tracking_error=te_annual, information_ratio=info_ratio)\n",
        "\n",
        "\n",
        "    # ========= OPTIMIZER =========\n",
        "def _annual_to_daily(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    return (1.0 + float(x))**(1/252) - 1.0\n",
        "\n",
        "def _build_cov_te_bits(rets: pd.DataFrame, bench_rets: Optional[pd.Series]):\n",
        "    \"\"\"Return Sigma (NxN), mu (N,), c (N,), var_b (scalar) for TE constraint.\"\"\"\n",
        "    Sigma = rets.cov().values\n",
        "    mu = rets.mean().values\n",
        "    if bench_rets is None:\n",
        "        return Sigma, mu, None, None\n",
        "    df = pd.concat([rets, bench_rets], axis=1).dropna()\n",
        "    A = df.iloc[:, :rets.shape[1]]  # asset returns\n",
        "    b = df.iloc[:, -1]              # benchmark returns\n",
        "    Sigma_te = A.cov().values\n",
        "    mu_te = A.mean().values\n",
        "    c = np.array([A.iloc[:, i].cov(b) for i in range(A.shape[1])])  # Cov(asset_i, bench)\n",
        "    var_b = float(b.var())\n",
        "    return Sigma_te, mu_te, c, var_b\n",
        "\n",
        "def _diag_feasibility(rets: pd.DataFrame,\n",
        "                      target_vol_a: Optional[float],\n",
        "                      te_max_a: Optional[float],\n",
        "                      bench_rets: Optional[pd.Series],\n",
        "                      w_min: float, w_max: float, long_only: bool,\n",
        "                      samples: int = 30000) -> dict:\n",
        "    \"\"\"\n",
        "    Probe the feasible set with random weights and report the best we can do.\n",
        "    Returns dict with min_vol, min_te, best_for_te (vol), best_for_vol (te).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(1)\n",
        "    Sigma = rets.cov().values\n",
        "    bench = None if bench_rets is None else bench_rets.reindex(rets.index).dropna()\n",
        "    c = None; var_b = None\n",
        "    if bench is not None:\n",
        "        A = pd.concat([rets, bench], axis=1).dropna()\n",
        "        A_only = A.iloc[:, :rets.shape[1]]\n",
        "        b = A.iloc[:, -1]\n",
        "        c = np.array([A_only.iloc[:, i].cov(b) for i in range(A_only.shape[1])])\n",
        "        var_b = float(b.var())\n",
        "\n",
        "    min_var = np.inf\n",
        "    min_te  = np.inf\n",
        "    best_vol_for_te = None\n",
        "    best_te_for_vol = None\n",
        "    N = rets.shape[1]\n",
        "    for _ in range(samples):\n",
        "        w = rng.dirichlet(np.ones(N))\n",
        "        over = w > w_max\n",
        "        if over.any():\n",
        "            excess = (w[over] - w_max).sum()\n",
        "            w[over] = w_max\n",
        "            w[~over] += excess * w[~over] / w[~over].sum()\n",
        "        w = np.clip(w, max(0.0 if long_only else -np.inf, w_min), w_max)\n",
        "        w = w / w.sum()\n",
        "\n",
        "        var_p = float(w @ Sigma @ w)\n",
        "        vol_a = np.sqrt(var_p) * np.sqrt(252)\n",
        "        te_a = None\n",
        "        if c is not None and var_b is not None:\n",
        "            te_var = var_p - 2 * float(c @ w) + var_b\n",
        "            te_a = np.sqrt(max(te_var, 0.0)) * np.sqrt(252)\n",
        "\n",
        "        if vol_a < min_var:\n",
        "            min_var = vol_a; best_te_for_vol = te_a\n",
        "        if te_a is not None and te_a < min_te:\n",
        "            min_te = te_a; best_vol_for_te = vol_a\n",
        "\n",
        "    out = {\n",
        "        \"min_achievable_vol\": float(min_var),\n",
        "        \"te_at_min_vol\": float(best_te_for_vol) if best_te_for_vol is not None else None,\n",
        "        \"min_achievable_te\": float(min_te) if np.isfinite(min_te) else None,\n",
        "        \"vol_at_min_te\": float(best_vol_for_te) if best_vol_for_te is not None else None,\n",
        "        \"target_vol\": target_vol_a,\n",
        "        \"target_te\": te_max_a\n",
        "    }\n",
        "    print(\"\\n[diagnostics] Feasibility probe:\")\n",
        "    for k,v in out.items():\n",
        "        if v is None: print(f\"  {k}: —\")\n",
        "        elif \"vol\" in k or \"te\" in k: print(f\"  {k}: {v:.2%}\")\n",
        "        else: print(f\"  {k}: {v}\")\n",
        "    return out\n",
        "\n",
        "def optimize_portfolio(\n",
        "    rets: pd.DataFrame,\n",
        "    rf_annual: float,\n",
        "    bench_rets: Optional[pd.Series],\n",
        "    opt_cfg: Dict,\n",
        "    prev_w: Optional[pd.Series] = None,\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Solve:\n",
        "      mode=\"max_return_for_vol\":   maximize (mu - rf)·w  s.t. w'Σw <= σ_target^2, TE<=cap\n",
        "      mode=\"min_vol_for_return\":   minimize w'Σw         s.t. (mu)·w >= μ_target,  TE<=cap\n",
        "      mode=\"mean_variance\":        maximize (mu - rf)·w - λ w'Σw  (+ constraints)\n",
        "    Constraints: sum w=1, w_min <= w <= w_max, long_only, optional cardinality.\n",
        "    Returns pd.Series of weights indexed by rets.columns.\n",
        "    \"\"\"\n",
        "    Sigma, mu, c, var_b = _build_cov_te_bits(rets, bench_rets)\n",
        "    rf_d = _annual_to_daily(rf_annual)\n",
        "    mu_d = mu.copy()\n",
        "    mu_excess = mu_d - rf_d\n",
        "\n",
        "    N = rets.shape[1]\n",
        "    w_min = float(opt_cfg.get(\"w_min\", 0.0))\n",
        "    w_max = float(opt_cfg.get(\"w_max\", 1.0))\n",
        "    long_only = bool(opt_cfg.get(\"long_only\", True))\n",
        "    mode = opt_cfg.get(\"mode\", \"max_return_for_vol\")\n",
        "    te_max_a = opt_cfg.get(\"te_max\", None)\n",
        "    te_max_d2 = None\n",
        "    if te_max_a is not None and bench_rets is not None:\n",
        "        te_max_d2 = (float(te_max_a) / np.sqrt(252))**2\n",
        "\n",
        "    target_vol_a = opt_cfg.get(\"target_vol\", None)\n",
        "    target_vol_d2 = (float(target_vol_a) / np.sqrt(252))**2 if target_vol_a is not None else None\n",
        "    target_ret_a = opt_cfg.get(\"target_return\", None)\n",
        "    target_ret_d = _annual_to_daily(target_ret_a) if target_ret_a is not None else None\n",
        "\n",
        "    card = opt_cfg.get(\"cardinality\", None)\n",
        "    # Treat 0/False/None as \"no cardinality\"\n",
        "    if card in (None, 0, False, \"none\", \"None\"):\n",
        "        card = None\n",
        "\n",
        "    min_pos = float(opt_cfg.get(\"min_position\", 0.0))\n",
        "    risk_aversion = float(opt_cfg.get(\"risk_aversion\", 10.0))\n",
        "\n",
        "    # Try cvxpy first\n",
        "    try:\n",
        "        import cvxpy as cp\n",
        "        w = cp.Variable(N)\n",
        "        cons = [cp.sum(w) == 1]\n",
        "\n",
        "        # ----- Previous weights (optional, for turnover control) -----\n",
        "        # Single name used consistently: w_prev (numpy array) or None.\n",
        "        w_prev = None\n",
        "\n",
        "        # Option A: caller passes a pandas Series in opt_cfg[\"prev_weights\"] (index=tickers).\n",
        "        prev_series = opt_cfg.get(\"prev_weights\", None)\n",
        "\n",
        "        # Option B: load from CSV (index=tickers, first column=weight).\n",
        "        if prev_series is None:\n",
        "            prev_path = opt_cfg.get(\"prev_weights_file\", None)\n",
        "            if prev_path:\n",
        "                try:\n",
        "                    tmp = pd.read_csv(prev_path, index_col=0).iloc[:, 0]\n",
        "                    prev_series = tmp\n",
        "                except Exception as _e:\n",
        "                    print(f\"[optimizer] Couldn't read prev_weights_file: {_e}\")\n",
        "\n",
        "        if prev_series is not None:\n",
        "            w_prev = np.zeros(N)\n",
        "            for i, c in enumerate(rets.columns):\n",
        "                if c in prev_series.index:\n",
        "                    w_prev[i] = float(prev_series.loc[c])\n",
        "            s = w_prev.sum()\n",
        "            if s > 0:\n",
        "                w_prev = w_prev / s\n",
        "            else:\n",
        "                w_prev = None  # nothing matched; disable turnover logic\n",
        "\n",
        "        # ----- Group constraints (adjust caps to taste) -----\n",
        "        groups = _group_indices(list(rets.columns))\n",
        "        if groups[\"equities\"]:\n",
        "            cons += [cp.sum(w[groups[\"equities\"]]) <= 0.80]   # <= 80% equities\n",
        "        if groups[\"bonds\"]:\n",
        "            cons += [cp.sum(w[groups[\"bonds\"]]) >= 0.10,      # 10–60% bonds\n",
        "                     cp.sum(w[groups[\"bonds\"]]) <= 0.60]\n",
        "        if groups[\"commodities\"]:\n",
        "            cons += [cp.sum(w[groups[\"commodities\"]]) <= 0.20]  # <= 20% commodities\n",
        "        if groups[\"crypto\"]:\n",
        "            cons += [cp.sum(w[groups[\"crypto\"]]) <= 0.05]       # <= 5% crypto\n",
        "\n",
        "                # inside optimize_portfolio after groups=...\n",
        "        inv = groups.get(\"inverse_equity\", [])\n",
        "        if inv:\n",
        "            # choose one:\n",
        "            # (a) hard ban\n",
        "            # cons += [cp.sum(w[inv]) == 0]\n",
        "            # (b) small cap, e.g. <= 5% aggregate\n",
        "            cons += [cp.sum(w[inv]) <= 0.05]\n",
        "\n",
        "        # ----- Turnover constraint (optional) -----\n",
        "        tmax = opt_cfg.get(\"turnover_max\", None)   # e.g., 0.20 for 20%\n",
        "        if tmax is not None and w_prev is not None:\n",
        "            cons += [cp.norm1(w - w_prev) <= float(tmax)]\n",
        "\n",
        "        if long_only:\n",
        "            cons += [w >= 0]\n",
        "        cons += [w >= w_min, w <= w_max]\n",
        "\n",
        "        var_port = cp.quad_form(w, Sigma)\n",
        "\n",
        "        # tracking error constraint if requested and benchmark given\n",
        "        if te_max_d2 is not None and c is not None and var_b is not None:\n",
        "            te_var = var_port - 2* (c @ w) + var_b\n",
        "            cons += [te_var <= te_max_d2]\n",
        "\n",
        "        # cardinality (mixed-integer); only enable if asked\n",
        "        solver = None\n",
        "        if card is not None:\n",
        "            z = cp.Variable(N, boolean=True)\n",
        "            cons += [w <= w_max * z]\n",
        "            if min_pos > 0:\n",
        "                cons += [w >= min_pos * z]\n",
        "            cons += [cp.sum(z) <= int(card)]\n",
        "            solver = cp.ECOS_BB  # bundled MILP; can be slow on big N\n",
        "\n",
        "        # penalties\n",
        "        l2_ridge = float(opt_cfg.get(\"l2_ridge\", 0.0))\n",
        "        tc_bps   = float(opt_cfg.get(\"tc_bps\", 0.0))\n",
        "        ridge = l2_ridge * cp.sum_squares(w) if l2_ridge > 0 else 0.0\n",
        "        tc_pen = 0.0\n",
        "        if tc_bps > 0 and w_prev is not None:\n",
        "            # rough: expected one-shot turnover cost in returns space\n",
        "            tc_pen = (tc_bps / 1e4) * cp.norm1(w - w_prev)\n",
        "\n",
        "\n",
        "        # objective / extra constraints by mode\n",
        "        if mode == \"max_return_for_vol\":\n",
        "            # example: constrain volatility\n",
        "            if target_vol_d2 is None:\n",
        "                raise ValueError(\"target_vol must be set for mode='max_return_for_vol'\")\n",
        "            cons += [var_port <= target_vol_d2]\n",
        "            obj = cp.Maximize(mu_excess @ w - ridge - tc_pen)\n",
        "\n",
        "        elif mode == \"min_vol_for_return\":\n",
        "            if target_ret_d is None:\n",
        "                raise ValueError(\"target_return must be set for mode='min_vol_for_return'\")\n",
        "            cons += [mu_d @ w >= target_ret_d]\n",
        "            obj = cp.Minimize(var_port + ridge + tc_pen)\n",
        "\n",
        "        elif mode == \"mean_variance\":\n",
        "            obj = cp.Maximize(mu_excess @ w - risk_aversion * var_port - ridge - tc_pen)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown optimizer mode.\")\n",
        "\n",
        "        prob = cp.Problem(obj, cons)\n",
        "\n",
        "        if solver is not None:\n",
        "            # Cardinality case → needs ECOS_BB (MILP)\n",
        "            prob.solve(solver=solver, max_iters=10_000, verbose=False)\n",
        "        else:\n",
        "            # Continuous case → prefer OSQP; fall back to SCS\n",
        "            try:\n",
        "                prob.solve(solver=cp.OSQP, verbose=False)\n",
        "            except Exception:\n",
        "                prob.solve(solver=cp.SCS, verbose=False)\n",
        "\n",
        "        if w.value is None:\n",
        "            raise RuntimeError(\"Optimizer failed to find a solution.\")\n",
        "\n",
        "        w_opt = np.asarray(w.value).ravel()\n",
        "        # clean tiny negatives due to numerical noise\n",
        "        w_opt[np.abs(w_opt) < 1e-10] = 0.0\n",
        "        # re-normalize if necessary\n",
        "        if not np.isclose(w_opt.sum(), 1.0):\n",
        "            w_opt = w_opt / w_opt.sum()\n",
        "\n",
        "        return pd.Series(w_opt, index=rets.columns, name=\"opt_weight\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[optimizer] cvxpy unavailable or failed ({e}). Using random-search fallback…\")\n",
        "\n",
        "    # ---- Fallback: random search on the simplex with box & (optional) TE/vol/ret constraints ----\n",
        "    rng = np.random.default_rng(0)\n",
        "    S = int(opt_cfg.get(\"random_samples\", 30000))\n",
        "    best_score = -1e18\n",
        "    best_w = None\n",
        "\n",
        "    # precompute for speed\n",
        "    Sigma = np.asarray(Sigma)\n",
        "    mu_d = np.asarray(mu_d)\n",
        "    c_vec = np.asarray(c) if c is not None else None\n",
        "\n",
        "    for _ in range(S):\n",
        "        # Dirichlet sample, then enforce box constraints (projective clipping)\n",
        "        raw = rng.dirichlet(alpha=np.ones(N))\n",
        "        w = raw.copy()\n",
        "\n",
        "        # box: if w_max too small, project greedily\n",
        "        over = w > w_max\n",
        "        if over.any():\n",
        "            excess = (w[over] - w_max).sum()\n",
        "            w[over] = w_max\n",
        "            w[~over] += excess * w[~over] / w[~over].sum()\n",
        "        w = np.clip(w, w_min, w_max)\n",
        "        w = w / w.sum()\n",
        "\n",
        "                # group caps in fallback\n",
        "        g = _group_indices(list(rets.columns))\n",
        "        def _sum(idx): return float(w[idx].sum()) if idx else 0.0\n",
        "        if g[\"equities\"]  and _sum(g[\"equities\"])  > 0.80:  continue\n",
        "        if g[\"bonds\"]     and ( _sum(g[\"bonds\"])   < 0.10 or _sum(g[\"bonds\"]) > 0.60 ): continue\n",
        "        if g[\"commodities\"] and _sum(g[\"commodities\"]) > 0.20: continue\n",
        "        if g[\"crypto\"]    and _sum(g[\"crypto\"])    > 0.05:  continue\n",
        "\n",
        "        # risk/TE/return checks\n",
        "        var_p = float(w @ Sigma @ w)\n",
        "        if target_vol_d2 is not None and var_p > target_vol_d2:\n",
        "            continue\n",
        "        if target_ret_d is not None and float(mu_d @ w) < target_ret_d:\n",
        "            continue\n",
        "        if te_max_d2 is not None and c_vec is not None and var_b is not None:\n",
        "            te_var = var_p - 2 * float(c_vec @ w) + var_b\n",
        "            if te_var > te_max_d2:\n",
        "                continue\n",
        "\n",
        "        # score\n",
        "        if mode == \"min_vol_for_return\":\n",
        "            score = -var_p\n",
        "        elif mode == \"mean_variance\":\n",
        "            score = float(mu_excess @ w) - risk_aversion * var_p\n",
        "        else:  # max_return_for_vol\n",
        "            score = float(mu_excess @ w)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_w = w\n",
        "\n",
        "        if best_w is None:\n",
        "            _diag_feasibility(\n",
        "                rets, target_vol_a=opt_cfg.get(\"target_vol\", None),\n",
        "                te_max_a=opt_cfg.get(\"te_max\", None),\n",
        "                bench_rets=bench_rets,\n",
        "                w_min=w_min, w_max=w_max, long_only=long_only,\n",
        "                samples=int(opt_cfg.get(\"random_samples\", 30000))\n",
        "            )\n",
        "            # Auto-relax strategy: drop TE, then raise vol cap, then switch to mean-variance\n",
        "    relaxed_msg = []\n",
        "    if te_max_d2 is not None:\n",
        "        te_max_d2 = None\n",
        "        relaxed_msg.append(\"removed TE cap\")\n",
        "    elif target_vol_d2 is not None:\n",
        "        target_vol_d2 *= 1.5\n",
        "        relaxed_msg.append(\"raised target_vol by 50%\")\n",
        "    else:\n",
        "        relaxed_msg.append(\"switched to mean_variance\")\n",
        "        # compute best mean-variance by score\n",
        "        risk_aversion = float(opt_cfg.get(\"risk_aversion\", 10.0))\n",
        "        best_score = -1e18\n",
        "        for _ in range(int(opt_cfg.get(\"random_samples\", 30000))):\n",
        "            w = np.random.default_rng().dirichlet(np.ones(N))\n",
        "            over = w > w_max\n",
        "            if over.any():\n",
        "                excess = (w[over] - w_max).sum()\n",
        "                w[over] = w_max\n",
        "                w[~over] += excess * w[~over] / w[~over].sum()\n",
        "            w = np.clip(w, w_min, w_max); w = w / w.sum()\n",
        "            var_p = float(w @ Sigma @ w)\n",
        "            score = float(mu_excess @ w) - risk_aversion * var_p\n",
        "            if score > best_score:\n",
        "                best_score = score; best_w = w\n",
        "        if best_w is not None:\n",
        "            print(f\"[optimizer] Auto-relax: {', '.join(relaxed_msg)}\")\n",
        "            return pd.Series(best_w, index=rets.columns, name=\"opt_weight\")\n",
        "    # If we relaxed TE or vol, try one more pass quickly\n",
        "    if best_w is None and (relaxed_msg and relaxed_msg[0] != \"switched to mean_variance\"):\n",
        "        rng = np.random.default_rng(0)\n",
        "        for _ in range(int(opt_cfg.get(\"random_samples\", 30000))):\n",
        "            w = rng.dirichlet(np.ones(N))\n",
        "            over = w > w_max\n",
        "            if over.any():\n",
        "                excess = (w[over] - w_max).sum()\n",
        "                w[over] = w_max\n",
        "                w[~over] += excess * w[~over] / w[~over].sum()\n",
        "            w = np.clip(w, w_min, w_max); w = w / w.sum()\n",
        "            var_p = float(w @ Sigma @ w)\n",
        "            if target_vol_d2 is not None and var_p > target_vol_d2:  # new higher cap\n",
        "                continue\n",
        "            score = float(mu_excess @ w)\n",
        "            if score > best_score:\n",
        "                best_score = score; best_w = w\n",
        "        if best_w is not None:\n",
        "            print(f\"[optimizer] Auto-relax: {', '.join(relaxed_msg)}\")\n",
        "            return pd.Series(best_w, index=rets.columns, name=\"opt_weight\")\n",
        "    raise RuntimeError(\"Random search couldn't find a feasible portfolio with current and relaxed constraints.\")\n",
        "    return pd.Series(best_w, index=rets.columns, name=\"opt_weight\")\n",
        "\n",
        "def summarize_portfolio_from_weights(rets: pd.DataFrame, w: pd.Series,\n",
        "                                     bench_rets: Optional[pd.Series], rf_annual: float):\n",
        "    \"\"\"Print quick stats for the weight vector.\"\"\"\n",
        "    pr = (rets @ w.values).dropna()\n",
        "    rf_d = _annual_to_daily(rf_annual)\n",
        "    ann_ret = (1 + pr).prod()**(252/len(pr)) - 1\n",
        "    ann_vol = pr.std() * np.sqrt(252)\n",
        "    sharpe = ((pr.mean() - rf_d) * 252) / (ann_vol if ann_vol != 0 else np.nan)\n",
        "\n",
        "    te = None\n",
        "    if bench_rets is not None:\n",
        "        ar = (pr - bench_rets.reindex(pr.index)).dropna()\n",
        "        te = ar.std() * np.sqrt(252)\n",
        "\n",
        "    print(\"\\n=== Optimized portfolio ===\")\n",
        "    print(\"Weights:\")\n",
        "\n",
        "    # format Series values as percentages (Series.to_string has no `formatters` arg)\n",
        "    s = w[w.abs() > 1e-8].sort_values(ascending=False)\n",
        "    labels = [f\"{t} ({TICKER_NAMES.get(t, 'Unknown')})\" for t in s.index]\n",
        "    s.index = labels\n",
        "    print(s.apply(lambda x: f\"{x:.2%}\").to_string())\n",
        "\n",
        "    print(f\"\\nStats: ann return {ann_ret:.2%} | vol {ann_vol:.2%} | Sharpe {sharpe:.2f}\"\n",
        "          + (\"\" if te is None else f\" | TE {te:.2%}\"))\n",
        "    return pr\n",
        "\n",
        "def print_port_vs_bench_table(port_metrics: Dict[str, float], bench_metrics: Dict[str, float], rel: Dict[str, float]):\n",
        "    # simple side-by-side print with formatting\n",
        "    def pct(x):  return \"nan\" if pd.isna(x) else f\"{x:.2%}\"\n",
        "    def dec(x):  return \"nan\" if pd.isna(x) else f\"{x:.2f}\"\n",
        "\n",
        "    print(\"\\n=== Portfolio vs Benchmark ===\")\n",
        "    rows = [\n",
        "        (\"Annualized return\", pct(port_metrics[\"ann_return\"]), pct(bench_metrics[\"ann_return\"])),\n",
        "        (\"Annualized volatility\", pct(port_metrics[\"ann_vol\"]), pct(bench_metrics[\"ann_vol\"])),\n",
        "        (\"Sharpe (rf)\", dec(port_metrics[\"sharpe\"]), dec(bench_metrics[\"sharpe\"])),\n",
        "        (\"Max drawdown\", pct(port_metrics[\"mdd\"]), pct(bench_metrics[\"mdd\"])),\n",
        "        (\"1d VaR 95%\", pct(port_metrics[\"var1d_95\"]), pct(bench_metrics[\"var1d_95\"])),\n",
        "        (\"21d VaR 95%\", pct(port_metrics[\"var21d_95\"]), pct(bench_metrics[\"var21d_95\"])),\n",
        "    ]\n",
        "    col_w = max(len(r[0]) for r in rows) + 2\n",
        "    print(f\"{'Metric'.ljust(col_w)}  Portfolio       Benchmark\")\n",
        "    for name, pv, bv in rows:\n",
        "        print(f\"{name.ljust(col_w)}  {pv:>12}    {bv:>12}\")\n",
        "\n",
        "    print(\"\\nRelative to benchmark:\")\n",
        "    print(f\"  Beta:              {dec(rel['beta'])}\")\n",
        "    print(f\"  Alpha (annual):    {pct(rel['alpha_annual'])}\")\n",
        "    print(f\"  Tracking Error:    {pct(rel['tracking_error'])}\")\n",
        "    print(f\"  Information Ratio: {dec(rel['information_ratio'])}\")\n",
        "\n",
        "def save_comparison_csv(port_metrics: Dict[str, float], bench_metrics: Dict[str, float], rel: Dict[str, float], out_dir: str):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    def rowify(d, tag): return [{\"Metric\": k, \"Value\": v, \"Which\": tag} for k, v in d.items()]\n",
        "    df = pd.DataFrame(rowify(port_metrics, \"portfolio\") + rowify(bench_metrics, \"benchmark\"))\n",
        "    df_rel = pd.DataFrame([rel])\n",
        "    df.to_csv(f\"{out_dir}/comparison_stats.csv\", index=False)\n",
        "    df_rel.to_csv(f\"{out_dir}/comparison_relative.csv\", index=False)\n",
        "\n",
        "\n",
        "# --- small helper for slicing by date window (module scope!) ---\n",
        "def _clip(s: pd.Series, start: pd.Timestamp, end: pd.Timestamp) -> pd.Series:\n",
        "    return s.loc[(s.index >= start) & (s.index <= end)]\n",
        "\n",
        "def analyze_crises(\n",
        "    port_rets: pd.Series,\n",
        "    equity_full: pd.Series,\n",
        "    windows: List[Dict[str, str]],\n",
        "    out_dir: str,\n",
        "    bench_rets_full: Optional[pd.Series] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each crisis window:\n",
        "      - Cum return (window)\n",
        "      - Max drawdown (within window)\n",
        "      - Worst day return\n",
        "      - Ann vol (window)\n",
        "      - Days to recover back to the equity level at window start (from start)\n",
        "      - (Optional) Benchmark cum return (window)\n",
        "    Saves CSV to outputs/crisis_report.csv and prints a compact table.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    for w in windows:\n",
        "        s = pd.Timestamp(w[\"start\"])\n",
        "        e = pd.Timestamp(w[\"end\"])\n",
        "\n",
        "        pr = _clip(port_rets, s, e).dropna()\n",
        "        if len(pr) < 5:\n",
        "            continue  # not enough data in this window\n",
        "\n",
        "        # Portfolio window metrics\n",
        "        eq_win = (1 + pr).cumprod()\n",
        "        mdd = (eq_win / eq_win.cummax() - 1).min()\n",
        "        cum_ret = eq_win.iloc[-1] - 1\n",
        "        worst_day = pr.min()\n",
        "        ann_vol = pr.std() * np.sqrt(252)\n",
        "\n",
        "        # Benchmark (optional, aligned to window dates; fill missing as 0% move)\n",
        "        bench_cum = np.nan\n",
        "        if bench_rets_full is not None:\n",
        "            br = bench_rets_full.reindex(pr.index).fillna(0.0)\n",
        "            bench_cum = (1 + br).prod() - 1\n",
        "\n",
        "        # Days to recover to start level, using full equity curve\n",
        "        # Find first date in full curve >= window start (in case of holiday mismatch)\n",
        "        idx_start = equity_full.index.searchsorted(s, side=\"left\")\n",
        "        if idx_start >= len(equity_full):\n",
        "            days_to_rec = np.nan\n",
        "            trade_days_to_rec = np.nan\n",
        "        else:\n",
        "            start_level = equity_full.iloc[idx_start]\n",
        "            # Look forward from window end\n",
        "            idx_end = equity_full.index.searchsorted(e, side=\"left\")\n",
        "            future = equity_full.iloc[idx_end:]\n",
        "            if len(future) == 0:\n",
        "                days_to_rec = np.nan\n",
        "                trade_days_to_rec = np.nan\n",
        "            else:\n",
        "                mask = future >= start_level\n",
        "                if mask.any():\n",
        "                    first_hit_pos = np.where(mask.values)[0][0]\n",
        "                    first_hit_date = future.index[first_hit_pos]\n",
        "                    days_to_rec = (first_hit_date - equity_full.index[idx_start]).days\n",
        "                    trade_days_to_rec = int(first_hit_pos)  # trading days\n",
        "                else:\n",
        "                    days_to_rec = np.nan\n",
        "                    trade_days_to_rec = np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"crisis\": w[\"name\"],\n",
        "            \"start\": s.date(),\n",
        "            \"end\": e.date(),\n",
        "            \"port_cum_return\": float(cum_ret),\n",
        "            \"port_mdd\": float(mdd),\n",
        "            \"port_worst_day\": float(worst_day),\n",
        "            \"port_ann_vol\": float(ann_vol),\n",
        "            \"days_to_recover_calendar\": days_to_rec if pd.notna(days_to_rec) else None,\n",
        "            \"days_to_recover_trading\": trade_days_to_rec if pd.notna(trade_days_to_rec) else None,\n",
        "            \"bench_cum_return\": float(bench_cum) if pd.notna(bench_cum) else None,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{out_dir}/crisis_report.csv\", index=False)\n",
        "\n",
        "    # Pretty print\n",
        "    if not df.empty:\n",
        "        def pct(x): return \"—\" if pd.isna(x) else f\"{x:.2%}\"\n",
        "        def dec(x): return \"—\" if pd.isna(x) else f\"{x:.2f}\"\n",
        "        print(\"\\n=== Crisis Scorecard (window stats) ===\")\n",
        "        header = f\"{'Crisis':28}  {'From':10}  {'To':10}  {'Port Ret':>9}  {'Bench Ret':>10}  {'MDD':>9}  {'Worst Day':>10}  {'AnnVol':>8}  {'Recov (trd)':>11}\"\n",
        "        print(header)\n",
        "        for _, r in df.iterrows():\n",
        "            print(f\"{r['crisis'][:28]:28}  {str(r['start']):10}  {str(r['end']):10}  \"\n",
        "                  f\"{pct(r['port_cum_return']):>9}  {pct(r['bench_cum_return']):>10}  \"\n",
        "                  f\"{pct(r['port_mdd']):>9}  {pct(r['port_worst_day']):>10}  \"\n",
        "                  f\"{pct(r['port_ann_vol']):>8}  \"\n",
        "                  f\"{('—' if pd.isna(r['days_to_recover_trading']) else int(r['days_to_recover_trading'])):>11}\")\n",
        "        print(f\"\\nSaved detailed CSV → {out_dir}/crisis_report.csv\")\n",
        "    else:\n",
        "        print(\"\\nCrisis scorecard: no windows overlapped your data range.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "    # ========= UNIVERSE LOADER + FILTERS =========\n",
        "def load_universe(cfg: dict) -> list:\n",
        "    \"\"\"\n",
        "    Returns a raw list of tickers from either a CSV ('universe_file')\n",
        "    or a preset ('universe_preset').\n",
        "    \"\"\"\n",
        "    if cfg.get(\"universe_file\"):\n",
        "        df = pd.read_csv(cfg[\"universe_file\"])\n",
        "        return df[\"ticker\"].dropna().astype(str).unique().tolist()\n",
        "\n",
        "    preset = cfg.get(\"universe_preset\")\n",
        "\n",
        "    if preset == \"broad_multi_asset\":\n",
        "        tickers = [\n",
        "            # US mega / quality\n",
        "            \"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"NVDA\",\"META\",\"TSLA\",\n",
        "            # Broad indices\n",
        "            \"SPY\",\"QQQ\",\"IWM\",\"EFA\",\"EEM\",\"EWJ\",\"VGK\",\"EWZ\",\"FXI\",\n",
        "            # Bonds\n",
        "            \"TLT\",\"IEF\",\"SHY\",\"HYG\",\"LQD\",\n",
        "            # Commodities\n",
        "            \"GLD\",\"SLV\",\"DBC\",\"USO\",\"UNG\",\n",
        "            # Real assets\n",
        "            \"VNQ\",\n",
        "            # Crypto\n",
        "            \"BTC-USD\",\"ETH-USD\",\"SOL-USD\",\n",
        "            # Leveraged equities\n",
        "            \"TQQQ\",\"UPRO\",\"SPXL\",    # 3x Nasdaq / S&P\n",
        "            \"SQQQ\",\"SPXS\",           # Inverse\n",
        "            # Leveraged commodities\n",
        "            \"UGL\",\"DGP\",             # 2x gold\n",
        "            \"GUSH\",\"NUGT\",\"BOIL\",    # Oil/gas/miners 2x/3x\n",
        "            # Volatility exposure\n",
        "            \"UVXY\",\"VXX\",\"VIXY\",\n",
        "            # High-beta equities / EM\n",
        "            \"ARKK\",\"XBI\",\"SOXL\",\"SMH\",\"KWEB\",\n",
        "            \"EEM\",\"EWZ\",\"FXI\",\"INDA\",\"RSX\",\n",
        "        ]\n",
        "\n",
        "    elif preset is None:\n",
        "        raise ValueError(\"You set neither 'tickers' nor 'universe_preset'/'universe_file'.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown universe_preset: {preset}\")\n",
        "\n",
        "    return tickers\n",
        "\n",
        "# ==== Portfolio correlation & diversification helpers ====\n",
        "\n",
        "def as_weight_series(w, cols) -> pd.Series:\n",
        "    \"\"\"Return weights as a pd.Series indexed by tickers.\"\"\"\n",
        "    if isinstance(w, pd.Series):\n",
        "        return w\n",
        "    return pd.Series(np.asarray(w, dtype=float), index=list(cols), name=\"weight\")\n",
        "\n",
        "def portfolio_avg_correlation(rets: pd.DataFrame, w: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Weighted average pairwise correlation:\n",
        "      sum_{i!=j} w_i w_j Corr_ij  /  (1 - sum w_i^2)\n",
        "    \"\"\"\n",
        "    rets = rets.loc[:, w.index]  # align\n",
        "    C = rets.corr().values\n",
        "    wv = w.values.astype(float)\n",
        "    # numerator: include i!=j\n",
        "    num = (wv[:, None] * wv[None, :] * C).sum() - (wv**2 * np.ones_like(wv)).sum()  # subtract diagonal terms (C_ii=1)\n",
        "    den = 1.0 - (wv**2).sum()\n",
        "    return float(num / den) if den > 1e-12 else np.nan\n",
        "\n",
        "def diversification_ratio(rets: pd.DataFrame, w: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    DR = (w' * asset_vols) / sqrt(w' Σ w)\n",
        "    where Σ is the sample covariance of daily returns.\n",
        "    \"\"\"\n",
        "    rets = rets.loc[:, w.index]\n",
        "    vols = rets.std().values                       # daily vols\n",
        "    Sigma = rets.cov().values                      # daily covariance\n",
        "    wv = w.values.astype(float)\n",
        "    top = float((wv * vols).sum())\n",
        "    bot = float(np.sqrt(wv @ Sigma @ wv))\n",
        "    return top / bot if bot > 0 else np.nan\n",
        "\n",
        "def class_level_corr(rets: pd.DataFrame, w: pd.Series) -> pd.DataFrame:\n",
        "    tickers = list(rets.columns)\n",
        "    groups = _group_indices(tickers)\n",
        "\n",
        "    # position -> class key\n",
        "    pos_to_cls = {}\n",
        "    for cls, idxs in groups.items():\n",
        "        for i in idxs:\n",
        "            pos_to_cls[i] = cls\n",
        "    for i in range(len(tickers)):\n",
        "        if i not in pos_to_cls:\n",
        "            pos_to_cls[i] = \"other\"\n",
        "\n",
        "    # collect by class (now includes inverse_equity)\n",
        "    class_members = {k: [] for k in CLASS_KEYS}\n",
        "    for i, t in enumerate(tickers):\n",
        "        class_members[pos_to_cls[i]].append(t)\n",
        "\n",
        "    class_series = {}\n",
        "    for cls_key in CLASS_KEYS:\n",
        "        names = class_members.get(cls_key, [])\n",
        "        if not names:\n",
        "            continue\n",
        "        w_sub = w.loc[names]\n",
        "        tot = float(w_sub.sum())\n",
        "        if tot <= 1e-12:\n",
        "            continue\n",
        "        w_norm = w_sub / tot\n",
        "        class_series[PRETTY_NAME[cls_key]] = (rets[names] @ w_norm.values)\n",
        "\n",
        "    if len(class_series) <= 1:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return pd.DataFrame(class_series).dropna().corr()\n",
        "\n",
        "def risk_contributions(rets: pd.DataFrame, w: pd.Series) -> pd.Series:\n",
        "    rets = rets.loc[:, w.index]\n",
        "    Sigma = rets.cov().values\n",
        "    wv = w.values.astype(float)\n",
        "    port_var = float(wv @ Sigma @ wv)\n",
        "    mrc = Sigma @ wv              # marginal risk contribution\n",
        "    rc = wv * mrc                 # total risk contribution\n",
        "    frac = rc / port_var          # % of portfolio variance\n",
        "    return pd.Series(frac, index=w.index, name=\"risk_contrib\")\n",
        "\n",
        "def volatility_target_weights(rets: pd.DataFrame, w: pd.Series,\n",
        "                              target_ann_vol=0.10, lookback=63,\n",
        "                              min_leverage=0.5, max_leverage=1.5,\n",
        "                              cash_ticker=\"SHY\") -> pd.Series:\n",
        "    # compute current realized vol\n",
        "    pr = (rets.loc[:, w.index] @ w.values).dropna()\n",
        "    cur_vol = pr.tail(lookback).std() * np.sqrt(252)\n",
        "    if pd.isna(cur_vol) or cur_vol == 0:\n",
        "        return w  # fallback\n",
        "\n",
        "    lev = np.clip(target_ann_vol / cur_vol, min_leverage, max_leverage)\n",
        "\n",
        "    w_scaled = w * lev\n",
        "    cash = 1.0 - w_scaled.sum()\n",
        "    w_out = w_scaled.copy()\n",
        "    if cash_ticker in w_out.index:\n",
        "        w_out[cash_ticker] = w_out.get(cash_ticker, 0.0) + cash\n",
        "    else:\n",
        "        w_out = pd.concat([w_out, pd.Series({cash_ticker: cash})])\n",
        "    return w_out\n",
        "\n",
        "def turnover_and_costs(w_new: pd.Series, prev_path=\"outputs/optimized_weights.csv\", tc_bps=5):\n",
        "    try:\n",
        "        w_prev = pd.read_csv(prev_path, index_col=0).iloc[:,0]\n",
        "        w_prev = w_prev.reindex(w_new.index).fillna(0.0)\n",
        "        to = float(np.abs(w_new - w_prev).sum())\n",
        "        cost = to * (tc_bps/1e4)\n",
        "        return to, cost\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def liquidity_check(tickers, start, end, w: pd.Series, aum_usd: float,\n",
        "                    lookback=20, max_pct_adv=0.05):\n",
        "    # pull close & volume together\n",
        "    df = yf.download(list(tickers), start=start, end=end, auto_adjust=False, progress=False)\n",
        "    px = df[\"Close\"].ffill().iloc[-1]               # latest close\n",
        "    adv_sh = df[\"Volume\"].rolling(lookback).mean().iloc[-1]\n",
        "    adv_usd = (adv_sh * px).dropna()\n",
        "\n",
        "    dollars = aum_usd * w.reindex(px.index).fillna(0.0)\n",
        "    breaches = (dollars / adv_usd).dropna()\n",
        "    breaches = breaches[breaches > max_pct_adv]\n",
        "    return breaches.sort_values(ascending=False)\n",
        "\n",
        "def target_to_orders(latest_prices: pd.Series, target_w: pd.Series,\n",
        "                     equity_value: float, current_qty: Optional[pd.Series]=None,\n",
        "                     lot_size: int = 1) -> pd.DataFrame:\n",
        "    target_w = target_w.reindex(latest_prices.index).fillna(0.0)\n",
        "    target_dollars = equity_value * target_w\n",
        "    target_qty = np.floor((target_dollars / latest_prices).fillna(0.0) / lot_size) * lot_size\n",
        "\n",
        "    if current_qty is None:\n",
        "        current_qty = pd.Series(0, index=latest_prices.index, dtype=float)\n",
        "\n",
        "    orders = (target_qty - current_qty).astype(int)\n",
        "    out = pd.DataFrame({\n",
        "        \"price\": latest_prices.round(4),\n",
        "        \"target_w\": target_w.round(6),\n",
        "        \"target_qty\": target_qty.astype(int),\n",
        "        \"current_qty\": current_qty.astype(int),\n",
        "        \"order_qty\": orders\n",
        "    })\n",
        "    return out[out[\"order_qty\"] != 0]\n",
        "\n",
        "def download_and_filter_universe(cfg: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Downloads prices for the universe, applies basic data quality filters,\n",
        "    caps to max_assets if requested, and returns a clean Close-price DataFrame.\n",
        "    \"\"\"\n",
        "    start, end = cfg[\"start\"], cfg[\"end\"]\n",
        "    min_hist = int(cfg.get(\"min_history_days\", 0))\n",
        "    min_cov  = float(cfg.get(\"min_coverage\", 0.0))\n",
        "    max_assets = cfg.get(\"max_assets\", None)\n",
        "\n",
        "    raw_tickers = load_universe(cfg)\n",
        "    print(f\"Universe (raw): {len(raw_tickers)} tickers\")\n",
        "\n",
        "    # Download in one call; get Close as DataFrame\n",
        "    px = yf.download(raw_tickers, start=start, end=end, auto_adjust=True, progress=False)[\"Close\"]\n",
        "    if isinstance(px, pd.Series):\n",
        "        px = px.to_frame()\n",
        "\n",
        "    # Forward-fill & drop fully empty columns\n",
        "    px = px.dropna(how=\"all\").ffill()\n",
        "\n",
        "    # Coverage filter\n",
        "    if min_cov > 0:\n",
        "        cov = px.notna().mean(axis=0)\n",
        "        keep = cov[cov >= min_cov].index.tolist()\n",
        "        px = px[keep]\n",
        "        print(f\"Filtered by coverage ≥ {min_cov:.0%}: {len(keep)} remain\")\n",
        "\n",
        "    # Minimum history filter (count of non-NA rows per column)\n",
        "    if min_hist > 0:\n",
        "        counts = px.notna().sum(axis=0)\n",
        "        keep = counts[counts >= min_hist].index.tolist()\n",
        "        px = px[keep]\n",
        "        print(f\"Filtered by min_history_days ≥ {min_hist}: {len(keep)} remain\")\n",
        "\n",
        "    # Drop any remaining columns that are entirely NA after filters\n",
        "    px = px.dropna(axis=1, how=\"all\")\n",
        "\n",
        "    # Cap universe size, if requested (keep the most liquid/proxied by fewer NA)\n",
        "    if max_assets is not None and len(px.columns) > max_assets:\n",
        "        # Rank by coverage desc, then by (recent) price variance desc to keep “interesting” ones\n",
        "        cov = px.notna().mean(axis=0)\n",
        "        vol = px.pct_change().std().fillna(0)\n",
        "        rank = (cov.rank(ascending=False) + vol.rank(ascending=False)).sort_values()\n",
        "        selected = rank.index[:max_assets].tolist()\n",
        "        px = px[selected]\n",
        "        print(f\"Capped to max_assets={max_assets}: {len(selected)} kept\")\n",
        "\n",
        "    # Final clean\n",
        "    px = px.ffill().dropna()\n",
        "    print(f\"Universe (final usable): {px.shape[1]} assets, {px.shape[0]} dates\")\n",
        "    return px\n",
        "\n",
        "def walk_forward_backtest(prices: pd.DataFrame, cfg: Dict, bench_rets: Optional[pd.Series]) -> pd.Series:\n",
        "    rets = prices.pct_change().dropna()\n",
        "    wf = cfg.get(\"walk_forward\", {})\n",
        "    freq = wf.get(\"rebalance_freq\",\"M\")\n",
        "    lookback = int(wf.get(\"lookback_days\",756))\n",
        "    tc_bps = float(wf.get(\"tc_bps\",5))\n",
        "    opt_cfg = cfg.get(\"optimizer\", {}).copy()\n",
        "    rf_annual = cfg[\"risk_free_annual\"]\n",
        "\n",
        "    # rebalance dates (use month/quarter ends)\n",
        "    rdates = rets.resample(freq).last().index\n",
        "    # ensure we have enough history before first trade\n",
        "    rdates = [d for d in rdates if d - rets.index[0] >= pd.Timedelta(days=lookback)]\n",
        "\n",
        "    w_cur = None\n",
        "    port_rets = []\n",
        "    for i, d in enumerate(rdates):\n",
        "        # window to estimate\n",
        "        train = rets.loc[(rets.index > d - pd.Timedelta(days=lookback)) & (rets.index <= d)]\n",
        "        if len(train) < 100:  # safety\n",
        "            continue\n",
        "\n",
        "        # align optional benchmark to train window\n",
        "        bench_train = None if bench_rets is None else bench_rets.reindex(train.index)\n",
        "\n",
        "        # pass previous weights to control turnover & costs\n",
        "        if w_cur is not None:\n",
        "            opt_cfg[\"prev_weights\"] = w_cur\n",
        "\n",
        "        w_new = optimize_portfolio(train, rf_annual, bench_train, opt_cfg)\n",
        "\n",
        "        # daily returns until next rebalance (OOS)\n",
        "        d_next = rdates[i+1] if i+1 < len(rdates) else rets.index[-1]\n",
        "        test = rets.loc[(rets.index > d) & (rets.index <= d_next)]\n",
        "        if test.empty:\n",
        "            w_cur = w_new\n",
        "            continue\n",
        "\n",
        "        # costs: approximate one-shot rebalance cost at start of period\n",
        "        if w_cur is None:\n",
        "            cost = 0.0\n",
        "        else:\n",
        "            turn = float(np.abs(w_new.reindex(w_cur.index, fill_value=0)-w_cur).sum())\n",
        "            cost = turn * (tc_bps/1e4)\n",
        "\n",
        "        pr = (test.loc[:, w_new.index] @ w_new.values)\n",
        "        # subtract one-shot cost on the first day of the period\n",
        "        if len(pr) > 0:\n",
        "            pr.iloc[0] -= cost\n",
        "        port_rets.append(pr)\n",
        "\n",
        "        w_cur = w_new\n",
        "\n",
        "    if not port_rets:\n",
        "        return pd.Series(dtype=float)\n",
        "\n",
        "    return pd.concat(port_rets).sort_index()"
      ],
      "metadata": {
        "id": "iyJbrgd48-_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(cfg: Dict):\n",
        "    start, end = cfg[\"start\"], cfg[\"end\"]\n",
        "\n",
        "    # 1) Universe\n",
        "    prices = download_and_filter_universe(cfg)\n",
        "    rf_annual = cfg[\"risk_free_annual\"]\n",
        "    mc_paths, mc_days = cfg[\"mc_paths\"], cfg[\"mc_days\"]\n",
        "    out_dir = cfg[\"save_folder\"]\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    make_plots = bool(cfg.get(\"make_charts\", True))\n",
        "    show_plots  = bool(cfg.get(\"show_plots\", True))\n",
        "\n",
        "    print(\"Got price history shape:\", prices.shape)\n",
        "\n",
        "    # 2) Returns matrix for optimizer/stats\n",
        "    rets_df = prices.pct_change().dropna()\n",
        "\n",
        "    # 3) Optimize (or fallback to equal-weight if optimizer disabled)\n",
        "    bench_series = None\n",
        "    if cfg.get(\"benchmark_ticker\"):\n",
        "        bpx = download_benchmark(cfg[\"benchmark_ticker\"], start, end)\n",
        "        bench_series = bpx.pct_change().reindex(rets_df.index)\n",
        "\n",
        "    if cfg.get(\"run_optimizer\", False):\n",
        "        opt_cfg  = cfg.get(\"optimizer\", {})\n",
        "        w_series = optimize_portfolio(rets_df[prices.columns], rf_annual, bench_series, opt_cfg)\n",
        "        w_series.to_csv(f\"{out_dir}/optimized_weights.csv\", header=True)\n",
        "        print(f\"Saved optimized weights → {out_dir}/optimized_weights.csv\")\n",
        "    else:\n",
        "        eq_w = np.repeat(1.0/len(prices.columns), len(prices.columns))\n",
        "        w_series = pd.Series(eq_w, index=prices.columns, name=\"weight\")\n",
        "\n",
        "    # 4) Compute portfolio stats using optimized weights\n",
        "    print(\"Computing portfolio stats...\")\n",
        "    res = compute_portfolio_stats(prices[w_series.index], w_series.values, rf_annual)\n",
        "\n",
        "    # 5) Optional: optimized summary, pie\n",
        "    pr_opt = summarize_portfolio_from_weights(rets_df[w_series.index], w_series, bench_series, rf_annual)\n",
        "    plot_asset_class_pie(w_series, out_path=f\"{out_dir}/asset_class_pie.png\")\n",
        "\n",
        "    # 6) Correlation & diversification summary (after optimizer)\n",
        "    avg_corr = portfolio_avg_correlation(rets_df[w_series.index], w_series)\n",
        "    div_ratio = diversification_ratio(rets_df[w_series.index], w_series)\n",
        "    cls_corr  = class_level_corr(rets_df[w_series.index], w_series)\n",
        "    print(f\"\\nWeighted average correlation: {avg_corr:.2f}\")\n",
        "    print(f\"Diversification ratio: {div_ratio:.2f}\")\n",
        "    if not cls_corr.empty:\n",
        "        print(\"Class-level correlation matrix:\")\n",
        "        print(cls_corr.round(2).to_string())\n",
        "        cls_corr.to_csv(f\"{out_dir}/class_level_correlation.csv\")\n",
        "    else:\n",
        "        print(\"Class-level correlation matrix: not enough non-zero class weights to compute.\")\n",
        "\n",
        "    # 7) Risk contributions, vol targeting, turnover, liquidity, orders\n",
        "    rc = risk_contributions(rets_df, w_series).sort_values(ascending=False)\n",
        "    print(\"\\nTop risk contributors (% of variance):\")\n",
        "    print((rc.head(10)*100).round(1).astype(str) + \"%\")\n",
        "    rc.to_csv(f\"{out_dir}/risk_contributions.csv\")\n",
        "\n",
        "    w_vol = volatility_target_weights(rets_df, w_series, target_ann_vol=0.10)\n",
        "    w_vol.to_csv(f\"{out_dir}/weights_vol_targeted.csv\", header=True)\n",
        "\n",
        "    to, cost = turnover_and_costs(w_series, tc_bps=CONFIG[\"optimizer\"][\"tc_bps\"])\n",
        "    if to is not None:\n",
        "        print(f\"\\nTurnover vs last run: {to:.2%} | est. one-shot cost: {cost:.2%}\")\n",
        "\n",
        "    AUM = 100_000.0\n",
        "    breaches = liquidity_check(w_series.index, CONFIG[\"start\"], CONFIG[\"end\"], w_series, AUM)\n",
        "    if not breaches.empty:\n",
        "        print(\"\\n⚠️ Liquidity breaches (position as % of ADV):\")\n",
        "        print((breaches*100).round(1).astype(str) + \"%\")\n",
        "\n",
        "    latest = yf.download(list(w_series.index), period=\"5d\", progress=False)[\"Close\"].ffill().iloc[-1]\n",
        "    orders = target_to_orders(latest, w_series, equity_value=100_000)\n",
        "    orders.to_csv(f\"{out_dir}/orders.csv\")\n",
        "    print(\"\\nOrder blotter preview:\")\n",
        "    print(orders.head(12).to_string())\n",
        "\n",
        "    # 8) Benchmark comparison\n",
        "    bench_ticker = cfg.get(\"benchmark_ticker\")\n",
        "    bench_rets = None\n",
        "    bench_rets_full = None\n",
        "    if bench_ticker:\n",
        "        bench_px = download_benchmark(bench_ticker, start, end)\n",
        "        bench_rets = bench_px.pct_change().reindex(res.rets.index)\n",
        "        aligned = pd.concat([res.rets, bench_rets], axis=1).dropna()\n",
        "        aligned.columns = [\"port\", \"bench\"]\n",
        "        port_rets_aligned = aligned[\"port\"]; bench_rets = aligned[\"bench\"]\n",
        "\n",
        "        bench_rets_full = bench_px.pct_change().reindex(res.rets.index)\n",
        "        bench_stats = compute_basic_stats_from_series(bench_rets, rf_annual)\n",
        "        port_stats_again = compute_basic_stats_from_series(port_rets_aligned, rf_annual)\n",
        "        rel = compute_relative_metrics(port_rets_aligned, bench_rets, rf_annual)\n",
        "        print_port_vs_bench_table(port_stats_again, bench_stats, rel)\n",
        "    else:\n",
        "        print(\"\\nNo benchmark_ticker set in CONFIG; skipping benchmark comparison.\")\n",
        "\n",
        "    # 9) Crisis scorecard\n",
        "    eq_full = (1 + res.rets).cumprod()\n",
        "    analyze_crises(\n",
        "        port_rets=res.rets,\n",
        "        equity_full=eq_full,\n",
        "        windows=cfg.get(\"crisis_windows\", []),\n",
        "        out_dir=out_dir,\n",
        "        bench_rets_full=bench_rets_full\n",
        "    )\n",
        "\n",
        "    # ----- Walk-forward OOS backtest (optional)\n",
        "    if \"walk_forward\" in cfg:\n",
        "        print(\"\\nRunning walk-forward backtest...\")\n",
        "        bench_for_wf = None\n",
        "        if cfg.get(\"benchmark_ticker\"):\n",
        "            bpx = download_benchmark(cfg[\"benchmark_ticker\"], cfg[\"start\"], cfg[\"end\"])\n",
        "            bench_for_wf = bpx.pct_change()\n",
        "\n",
        "        wf_rets = walk_forward_backtest(prices, cfg, bench_for_wf)\n",
        "        if not wf_rets.empty:\n",
        "            wf_stats = compute_basic_stats_from_series(wf_rets, cfg[\"risk_free_annual\"])\n",
        "            print(\"\\n=== Walk-forward (out-of-sample) stats ===\")\n",
        "            for k,v in wf_stats.items():\n",
        "                print(f\"{k:22}: {v:.4f}\" if \"sharpe\" in k.lower() else f\"{k:22}: {v:.2%}\")\n",
        "            # Save & plot\n",
        "            pd.DataFrame({\"wf_rets\": wf_rets}).to_csv(f\"{out_dir}/walk_forward_returns.csv\")\n",
        "            if make_plots and HAS_MPL:\n",
        "                set_mpl_style()\n",
        "                ((1+wf_rets).cumprod()).plot(lw=2)\n",
        "                plt.title(\"Walk-forward equity (OOS)\")\n",
        "                plt.xlabel(\"Date\"); plt.ylabel(\"Equity (start=1)\")\n",
        "                plt.savefig(f\"{out_dir}/walk_forward_equity.png\")\n",
        "                if show_plots: plt.show()\n",
        "\n",
        "    # 10) Monte Carlo & stress tests (use same weights)\n",
        "    print(\"\\nRunning Monte Carlo...\")\n",
        "    equity_curves, final_returns, stats = monte_carlo(res.mu_daily, res.cov_daily, w_series.values, mc_paths, mc_days)\n",
        "    print(\"MC expected 1y return: {:.2%} | median: {:.2%} | p5: {:.2%} | p95: {:.2%}\".format(\n",
        "        stats[\"expected_1y_return_mc\"], stats[\"median_1y_return_mc\"], stats[\"p5_1y_return_mc\"], stats[\"p95_1y_return_mc\"]\n",
        "    ))\n",
        "\n",
        "    print(\"\\n=== Simple 1-day stress tests ===\")\n",
        "    st = stress_tests(prices[w_series.index], w_series.values)\n",
        "    print(st.to_string(index=False, formatters={\"Portfolio 1-day P&L (%)\": \"{:.2f}\".format}))\n",
        "\n",
        "    # 11) Save & charts\n",
        "    save_outputs(res, stats, final_returns, out_dir)\n",
        "    if make_plots:\n",
        "        set_mpl_style()\n",
        "        plt.figure()\n",
        "        (1 + pr_opt).cumprod().plot(label=\"Optimized\", linewidth=2)\n",
        "        if bench_series is not None:\n",
        "            (1 + bench_series.fillna(0)).cumprod().plot(label=\"Benchmark\", linewidth=2)\n",
        "        plt.title(\"EQUITY CURVES — Current vs Optimized (and Benchmark)\")\n",
        "        plt.xlabel(\"Date\"); plt.ylabel(\"Equity (start=1.0)\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"{out_dir}/optimized_equity.png\")\n",
        "        if show_plots: plt.show()\n",
        "\n",
        "    with open(f\"{out_dir}/config.json\", \"w\") as f:\n",
        "        json.dump(cfg, f, indent=2)\n",
        "    print(f\"\\nConfig saved to {out_dir}/config.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = CONFIG.copy()\n",
        "if len(sys.argv) > 1 and sys.argv[1].endswith(\".json\"):\n",
        "        with open(sys.argv[1], \"r\") as f:\n",
        "            user_cfg = json.load(f)\n",
        "        cfg.update(user_cfg)\n",
        "main(cfg)"
      ],
      "metadata": {
        "id": "HBqQAsFulb8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}